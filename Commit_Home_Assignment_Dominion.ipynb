{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Offline Assignment"
      ],
      "metadata": {
        "id": "r516Fg2eTeQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an offline assigment meant for us in Commit to understand how you approach data science problems, and how you implement said approach.\n"
      ],
      "metadata": {
        "id": "dZWYvQInTHjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is 100% OK if you don't succeed finishing some of the sub-tasks- in fact, it is typical not get everything right.\n",
        "\n",
        "We wanted to give you the best chance to show us what you bring to the table no matter which field of data science interests you most.\n",
        "\n",
        "\n",
        "We have many tasks in Commit that are very similar to the task you see here.\n"
      ],
      "metadata": {
        "id": "XsByD0scTSB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important note- when you encounter a problem, all approaches that make sense are valid. Tradeoffs are the nature of the field, and it is likely that your considerations for tradeoffs will somewhat vary from those of the interviewer. As long as you can explain your considerations (or most of them), this is 100% fine**"
      ],
      "metadata": {
        "id": "OKIcTOQQTKQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Case - NLP"
      ],
      "metadata": {
        "id": "YbVCXmk6Tjtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Description"
      ],
      "metadata": {
        "id": "tzaq3LYEP1yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are given a task in the field of NLP.\n",
        "Our client, a large scientific corporation, is interested in using LLMs to improve its scientific work."
      ],
      "metadata": {
        "id": "j21W_GHHRIqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our First Task"
      ],
      "metadata": {
        "id": "_PeDhJnyRP02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of the scientific work involves making internet search for a given topic, and presenting only the important material to a researcher.\n",
        "\n",
        "\n",
        "For that, they would like you to mine a portion of the web (several web pages are enough) and create a vector database that you can query.\n",
        "\n",
        "\n",
        "Please mine several web pages (from ArXiv or Pubmed), extract the text from them, then split the text in a meaningful way, then build a vector database that one can query to retrieve the needed data.\n"
      ],
      "metadata": {
        "id": "YeWO_WglRUCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our Follow-up Task\n",
        "\n",
        "Now that you are done, the client wants more (clients almost always do).\n",
        "\n",
        "The client now wants to be able to ask complex questions (like â€œwhat are the dangerous medical conditions oh wise chatbotâ€) and still get an answer based on their data.\n",
        "\n",
        "\n",
        "Upon thinking, you came up with a solution- you will break the question into sub steps, query the vector database about each sub-step, and then compose an answer.\n",
        "\n",
        "Please create a flow that allows a person to ask complex questions and still get a reply based on their data. This flow should break down the question to the right substeps and then compose a good answer.  \n"
      ],
      "metadata": {
        "id": "iOa10DPqJV1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our Next Follow-up Task\n",
        "\n",
        "Now that you are done, the client is satisfied and wants to move from POC into production.\n",
        "\n",
        "Please harden your code and add as much of the items needed for code to be production-grade as you can.\n",
        "\n",
        "Examples are: log mockups, error handling, etc.\n",
        "\n",
        "Remember - the key here is to see that you know what to add and what to look for, implementation is secondary"
      ],
      "metadata": {
        "id": "vVYmVVCVJjBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Follow Up Task - Evaluation\n",
        "\n",
        "Now, the customer is pleased but they raised a question- how do I know that my model produces accurate results?\n",
        "\n",
        "Please think of a way to measure how accurate the model is.\n",
        "\n",
        "Please make sure that the measuring method actually measures the quality of the answer."
      ],
      "metadata": {
        "id": "azGNpCZ8JjGa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RgHp-7MWKHA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Web Mining & Vector Database\n",
        "\n",
        "## Architecture Diagram\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                        DATA INGESTION                          â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
        "â”‚  â”‚  ArXiv /  â”‚â”€â”€â”€â–¶â”‚  Web Scraper  â”‚â”€â”€â”€â–¶â”‚ Text Extractor â”‚        â”‚\n",
        "â”‚  â”‚  PubMed   â”‚    â”‚ (httpx async  â”‚    â”‚ (BeautifulSoup)â”‚        â”‚\n",
        "â”‚  â”‚  URLs     â”‚    â”‚  + rate limit)â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚                  â”‚\n",
        "â”‚                                               â–¼                  â”‚\n",
        "â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
        "â”‚                                    â”‚  Text Chunker     â”‚         â”‚\n",
        "â”‚                                    â”‚  (RecursiveChar   â”‚         â”‚\n",
        "â”‚                                    â”‚   TextSplitter)   â”‚         â”‚\n",
        "â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
        "â”‚                                             â–¼                    â”‚\n",
        "â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
        "â”‚                                    â”‚  Embedding Model  â”‚         â”‚\n",
        "â”‚                                    â”‚  (sentence-       â”‚         â”‚\n",
        "â”‚                                    â”‚   transformers)   â”‚         â”‚\n",
        "â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
        "â”‚                                             â–¼                    â”‚\n",
        "â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
        "â”‚                                    â”‚  ChromaDB Vector  â”‚         â”‚\n",
        "â”‚                                    â”‚  Store            â”‚         â”‚\n",
        "â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                      QUERY PIPELINE                             â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
        "â”‚  â”‚  User     â”‚â”€â”€â”€â–¶â”‚ Query        â”‚â”€â”€â”€â–¶â”‚ Question        â”‚        â”‚\n",
        "â”‚  â”‚  Question â”‚    â”‚ Decomposer   â”‚    â”‚ Decomposition   â”‚        â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ (LLM)        â”‚    â”‚ (Sub-queries)   â”‚        â”‚\n",
        "â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
        "â”‚                                              â–¼                   â”‚\n",
        "â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
        "â”‚                                    â”‚ Vector DB Query  â”‚          â”‚\n",
        "â”‚                                    â”‚ (per sub-query)  â”‚          â”‚\n",
        "â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
        "â”‚                                             â–¼                    â”‚\n",
        "â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
        "â”‚                                    â”‚ Answer Composer  â”‚          â”‚\n",
        "â”‚                                    â”‚ (LLM synthesis)  â”‚          â”‚\n",
        "â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
        "â”‚                                             â–¼                    â”‚\n",
        "â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
        "â”‚                                    â”‚  Final Answer    â”‚          â”‚\n",
        "â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ],
      "metadata": {
        "id": "ktqaKLDeGAx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your solution here\n",
        "# Install dependencies\n",
        "!pip install -q httpx beautifulsoup4 chromadb sentence-transformers langchain langchain-text-splitters openai tiktoken"
      ],
      "metadata": {
        "id": "D0QKPuWSQ-My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import httpx\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Optional\n",
        "import hashlib"
      ],
      "metadata": {
        "id": "QqowGJFfGSPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Web Mining â€” Scrape ArXiv & PubMed Pages"
      ],
      "metadata": {
        "id": "JsDRT2-ZGX2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target papers\n",
        "# Topic: Medical AI / dangerous medical conditions (aligns with Task 2's example query)\n",
        "ARXIV_URLS = [\n",
        "    \"https://arxiv.org/abs/2308.14089\",  # Large Language Models in Medicine\n",
        "    \"https://arxiv.org/abs/2305.09617\",  # Clinical LLM applications\n",
        "    \"https://arxiv.org/abs/2307.14334\",  # Med-PaLM\n",
        "    \"https://arxiv.org/abs/2212.13138\",  # ChatGPT in healthcare\n",
        "    \"https://arxiv.org/abs/2306.05052\",  # Biomedical NLP\n",
        "]\n",
        "\n",
        "PUBMED_URLS = [\n",
        "    \"https://pubmed.ncbi.nlm.nih.gov/37548974/\",\n",
        "    \"https://pubmed.ncbi.nlm.nih.gov/36930957/\",\n",
        "]\n",
        "\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Research Bot; educational purposes)\"}\n",
        "\n",
        "\n",
        "async def scrape_arxiv_page(client: httpx.AsyncClient, url: str) -> Dict[str, str]:\n",
        "    \"\"\"Scrape an ArXiv abstract page and extract title + abstract text.\"\"\"\n",
        "    response = await client.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    title_tag = soup.find(\"h1\", class_=\"title\")\n",
        "    title = title_tag.get_text(strip=True).replace(\"Title:\", \"\").strip() if title_tag else \"Unknown Title\"\n",
        "\n",
        "    abstract_tag = soup.find(\"blockquote\", class_=\"abstract\")\n",
        "    abstract = abstract_tag.get_text(strip=True).replace(\"Abstract:\", \"\").strip() if abstract_tag else \"\"\n",
        "\n",
        "    # Try to get full paper text from HTML version\n",
        "    html_url = url.replace(\"/abs/\", \"/html/\")\n",
        "    full_text = \"\"\n",
        "    try:\n",
        "        html_response = await client.get(html_url)\n",
        "        if html_response.status_code == 200:\n",
        "            html_soup = BeautifulSoup(html_response.text, \"html.parser\")\n",
        "            article = html_soup.find(\"article\") or html_soup.find(\"div\", class_=\"ltx_page_content\")\n",
        "            if article:\n",
        "                for ref in article.find_all([\"section\"], class_=lambda x: x and \"bib\" in str(x).lower()):\n",
        "                    ref.decompose()\n",
        "                full_text = article.get_text(separator=\" \", strip=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    content = full_text if full_text else abstract\n",
        "    return {\"title\": title, \"url\": url, \"content\": content, \"source\": \"arxiv\"}\n",
        "\n",
        "\n",
        "async def scrape_pubmed_page(client: httpx.AsyncClient, url: str) -> Dict[str, str]:\n",
        "    \"\"\"Scrape a PubMed page for title and abstract.\"\"\"\n",
        "    response = await client.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    title_tag = soup.find(\"h1\", class_=\"heading-title\")\n",
        "    title = title_tag.get_text(strip=True) if title_tag else \"Unknown Title\"\n",
        "\n",
        "    abstract_tag = soup.find(\"div\", class_=\"abstract-content\")\n",
        "    abstract = abstract_tag.get_text(strip=True) if abstract_tag else \"\"\n",
        "\n",
        "    return {\"title\": title, \"url\": url, \"content\": abstract, \"source\": \"pubmed\"}\n",
        "\n",
        "\n",
        "async def scrape_all_pages() -> List[Dict[str, str]]:\n",
        "    \"\"\"Scrape all pages concurrently with semaphore-based rate limiting.\"\"\"\n",
        "    semaphore = asyncio.Semaphore(3)  # Max 3 concurrent requests\n",
        "\n",
        "    async def _scrape_with_limit(coro, url: str, source: str):\n",
        "        async with semaphore:\n",
        "            try:\n",
        "                doc = await coro\n",
        "                print(f\"  âœ“ [{source}] {doc['title'][:60]}... ({len(doc['content'])} chars)\")\n",
        "                return doc\n",
        "            except Exception as e:\n",
        "                print(f\"  âœ— Failed to scrape {url}: {e}\")\n",
        "                return None\n",
        "\n",
        "    async with httpx.AsyncClient(\n",
        "        headers=HEADERS,\n",
        "        follow_redirects=True,\n",
        "        timeout=httpx.Timeout(15.0),\n",
        "    ) as client:\n",
        "        tasks = []\n",
        "        print(\"Scraping ArXiv & PubMed pages...\")\n",
        "        for url in ARXIV_URLS:\n",
        "            tasks.append(_scrape_with_limit(scrape_arxiv_page(client, url), url, \"arxiv\"))\n",
        "        for url in PUBMED_URLS:\n",
        "            tasks.append(_scrape_with_limit(scrape_pubmed_page(client, url), url, \"pubmed\"))\n",
        "\n",
        "        results = await asyncio.gather(*tasks)\n",
        "        documents = [doc for doc in results if doc is not None]\n",
        "\n",
        "    print(f\"\\nTotal documents scraped: {len(documents)}\")\n",
        "    return documents\n",
        "\n",
        "\n",
        "# In Jupyter/Colab, the event loop is already running â€” use await directly\n",
        "documents = await scrape_all_pages()"
      ],
      "metadata": {
        "id": "l6b78GOJGU_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Text Splitting / Chunking\n",
        "\n",
        "**Design Decisions:**\n",
        "- `RecursiveCharacterTextSplitter` splits on natural boundaries (paragraphs â†’ sentences â†’ words)\n",
        "- Chunk size of 500 chars with 100 char overlap â€” good balance for scientific text\n",
        "- Each chunk retains metadata (source URL, title) for traceability"
      ],
      "metadata": {
        "id": "H1g5_YknGgaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_documents(documents: List[Dict], chunk_size: int = 500, chunk_overlap: int = 100) -> List[Dict]:\n",
        "    \"\"\"Split documents into meaningful chunks with metadata.\"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "        length_function=len,\n",
        "    )\n",
        "\n",
        "    all_chunks = []\n",
        "    for doc in documents:\n",
        "        text = re.sub(r'\\s+', ' ', doc[\"content\"]).strip()\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        chunks = splitter.split_text(text)\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk_id = hashlib.md5(f\"{doc['url']}_{i}\".encode()).hexdigest()\n",
        "            all_chunks.append({\n",
        "                \"id\": chunk_id,\n",
        "                \"text\": chunk,\n",
        "                \"metadata\": {\n",
        "                    \"source_url\": doc[\"url\"],\n",
        "                    \"title\": doc[\"title\"],\n",
        "                    \"source\": doc[\"source\"],\n",
        "                    \"chunk_index\": i,\n",
        "                    \"total_chunks\": len(chunks),\n",
        "                }\n",
        "            })\n",
        "\n",
        "    return all_chunks\n",
        "\n",
        "\n",
        "chunks = chunk_documents(documents)\n",
        "print(f\"Total chunks created: {len(chunks)}\")\n",
        "print(f\"\\nSample chunk:\")\n",
        "print(f\"  ID: {chunks[0]['id']}\")\n",
        "print(f\"  Text: {chunks[0]['text'][:200]}...\")\n",
        "print(f\"  Metadata: {chunks[0]['metadata']}\")"
      ],
      "metadata": {
        "id": "7xUvnUDjGVC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Build Vector Database with ChromaDB"
      ],
      "metadata": {
        "id": "eWSbrdACGsbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize embedding model (all-MiniLM-L6-v2: fast + good quality)\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize ChromaDB (in-memory for POC)\n",
        "chroma_client = chromadb.Client()\n",
        "collection = chroma_client.create_collection(\n",
        "    name=\"scientific_papers\",\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "\n",
        "# Embed and insert chunks in batches\n",
        "print(\"Embedding and inserting chunks...\")\n",
        "batch_size = 50\n",
        "for i in range(0, len(chunks), batch_size):\n",
        "    batch = chunks[i:i + batch_size]\n",
        "    texts = [c[\"text\"] for c in batch]\n",
        "    ids = [c[\"id\"] for c in batch]\n",
        "    metadatas = [c[\"metadata\"] for c in batch]\n",
        "    embeddings = embedding_model.encode(texts).tolist()\n",
        "    collection.add(documents=texts, embeddings=embeddings, metadatas=metadatas, ids=ids)\n",
        "    print(f\"  Inserted batch {i // batch_size + 1} ({len(batch)} chunks)\")\n",
        "\n",
        "print(f\"\\nVector database ready. Total documents: {collection.count()}\")"
      ],
      "metadata": {
        "id": "t8mD3yT-Gua9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_vector_db(query: str, n_results: int = 3) -> List[Dict]:\n",
        "    \"\"\"Query the vector database and return relevant chunks.\"\"\"\n",
        "    query_embedding = embedding_model.encode([query]).tolist()\n",
        "    results = collection.query(\n",
        "        query_embeddings=query_embedding,\n",
        "        n_results=n_results,\n",
        "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "    )\n",
        "    return [\n",
        "        {\n",
        "            \"text\": results[\"documents\"][0][i],\n",
        "            \"metadata\": results[\"metadatas\"][0][i],\n",
        "            \"distance\": results[\"distances\"][0][i],\n",
        "        }\n",
        "        for i in range(len(results[\"ids\"][0]))\n",
        "    ]\n",
        "\n",
        "\n",
        "# Test query\n",
        "test_results = query_vector_db(\"What are the risks of using AI in medical diagnosis?\")\n",
        "print(\"Query: 'What are the risks of using AI in medical diagnosis?'\\n\")\n",
        "for i, r in enumerate(test_results):\n",
        "    print(f\"Result {i+1} (distance: {r['distance']:.4f}):\")\n",
        "    print(f\"  Source: {r['metadata']['title'][:60]}\")\n",
        "    print(f\"  Text: {r['text'][:200]}...\\n\")"
      ],
      "metadata": {
        "id": "dM93P2ncGueN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T--ZavMOKZUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Task 2: Complex Question Answering via Query Decomposition\n",
        "\n",
        "### Approach: Multi-Step RAG with Query Decomposition\n",
        "\n",
        "1. **Decompose** the complex question into atomic sub-questions using an LLM\n",
        "2. **Retrieve** relevant chunks for each sub-question from the vector DB\n",
        "3. **Synthesize** a final answer using all retrieved context + the original question"
      ],
      "metadata": {
        "id": "nxW5R61ZHAiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set your API key (use env vars or secrets manager in production)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"  # Replace with your key\n",
        "llm_client = OpenAI()\n",
        "\n",
        "\n",
        "def decompose_question(question: str) -> List[str]:\n",
        "    \"\"\"Break a complex question into simpler, searchable sub-questions via LLM.\"\"\"\n",
        "    prompt = f\"\"\"You are a research assistant. Given a complex question, break it down into\n",
        "2-5 simpler, specific sub-questions that can each be answered independently by searching\n",
        "a scientific literature database.\n",
        "\n",
        "Rules:\n",
        "- Each sub-question should be self-contained and searchable\n",
        "- Cover all aspects of the original question\n",
        "- Return ONLY a JSON list of strings, no other text\n",
        "\n",
        "Complex question: \"{question}\"\n",
        "\n",
        "Sub-questions (JSON list):\"\"\"\n",
        "\n",
        "    response = llm_client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.0,\n",
        "        max_tokens=500,\n",
        "    )\n",
        "\n",
        "    raw = response.choices[0].message.content.strip()\n",
        "    try:\n",
        "        return json.loads(raw)\n",
        "    except json.JSONDecodeError:\n",
        "        match = re.search(r'\\[.*\\]', raw, re.DOTALL)\n",
        "        if match:\n",
        "            return json.loads(match.group())\n",
        "        return [question]\n",
        "\n",
        "\n",
        "def retrieve_for_subquestions(sub_questions: List[str], n_results_per_query: int = 3) -> List[Dict]:\n",
        "    \"\"\"Retrieve relevant chunks for each sub-question, deduplicated.\"\"\"\n",
        "    all_results = []\n",
        "    seen_ids = set()\n",
        "    for sq in sub_questions:\n",
        "        results = query_vector_db(sq, n_results=n_results_per_query)\n",
        "        for r in results:\n",
        "            chunk_id = r[\"metadata\"][\"source_url\"] + \"_\" + str(r[\"metadata\"][\"chunk_index\"])\n",
        "            if chunk_id not in seen_ids:\n",
        "                seen_ids.add(chunk_id)\n",
        "                r[\"sub_question\"] = sq\n",
        "                all_results.append(r)\n",
        "    return all_results\n",
        "\n",
        "\n",
        "def synthesize_answer(question: str, sub_questions: List[str], context_chunks: List[Dict]) -> str:\n",
        "    \"\"\"Synthesize a final answer from retrieved context using LLM.\"\"\"\n",
        "    context_parts = [\n",
        "        f\"[Source {i+1}: {chunk['metadata']['title']}]\\n{chunk['text']}\"\n",
        "        for i, chunk in enumerate(context_chunks)\n",
        "    ]\n",
        "    context_str = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    prompt = f\"\"\"You are a scientific research assistant. Answer the user's question based ONLY\n",
        "on the provided context from scientific papers. If the context doesn't contain enough information\n",
        "to fully answer, say so explicitly.\n",
        "\n",
        "The question was broken into these sub-questions for research:\n",
        "{json.dumps(sub_questions, indent=2)}\n",
        "\n",
        "Retrieved Context:\n",
        "{context_str}\n",
        "\n",
        "Original Question: {question}\n",
        "\n",
        "Please provide a comprehensive answer with citations to the source papers where applicable.\n",
        "Answer:\"\"\"\n",
        "\n",
        "    response = llm_client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.2,\n",
        "        max_tokens=1000,\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "def ask_complex_question(question: str) -> Dict:\n",
        "    \"\"\"End-to-end pipeline: decompose â†’ retrieve â†’ synthesize.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    print(\"\\nðŸ“‹ Step 1: Decomposing question...\")\n",
        "    sub_questions = decompose_question(question)\n",
        "    for i, sq in enumerate(sub_questions):\n",
        "        print(f\"  Sub-Q{i+1}: {sq}\")\n",
        "\n",
        "    print(\"\\nðŸ” Step 2: Retrieving relevant context...\")\n",
        "    context_chunks = retrieve_for_subquestions(sub_questions)\n",
        "    print(f\"  Retrieved {len(context_chunks)} unique chunks\")\n",
        "\n",
        "    print(\"\\nðŸ¤– Step 3: Synthesizing answer...\")\n",
        "    answer = synthesize_answer(question, sub_questions, context_chunks)\n",
        "    print(f\"\\nðŸ“ Answer:\\n{answer}\")\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"sub_questions\": sub_questions,\n",
        "        \"context_chunks\": context_chunks,\n",
        "        \"answer\": answer,\n",
        "    }"
      ],
      "metadata": {
        "id": "uuSamCj1HD-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with the example complex question\n",
        "result = ask_complex_question(\n",
        "    \"What are the dangerous medical conditions that AI systems might misdiagnose, \"\n",
        "    \"and what safety measures should be put in place?\"\n",
        ")"
      ],
      "metadata": {
        "id": "XQHD7eeGHD7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with another complex question\n",
        "result2 = ask_complex_question(\n",
        "    \"How do large language models compare to traditional NLP methods in \"\n",
        "    \"biomedical text mining, and what are the limitations of each approach?\"\n",
        ")"
      ],
      "metadata": {
        "id": "gMNWUPeCHD4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yFoYoiX7KXb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Task 3: Production-Hardened Code\n",
        "\n",
        "### What production-grade code needs:\n",
        "1. **Structured logging** with levels (DEBUG, INFO, WARNING, ERROR)\n",
        "2. **Error handling** with retries and graceful degradation\n",
        "3. **Configuration management** (no hardcoded values)\n",
        "4. **Input validation and sanitization**\n",
        "5. **Rate limiting and circuit breakers**\n",
        "6. **Health checks and monitoring hooks**\n",
        "7. **Type hints and documentation**\n",
        "8. **Idempotent operations** (safe to retry)\n",
        "9. **Metrics collection** for observability\n",
        "10. **Graceful shutdown handling**"
      ],
      "metadata": {
        "id": "OhbPh7mbHMym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "import hashlib\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import asyncio\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Optional, Any, Tuple\n",
        "from functools import wraps\n",
        "from datetime import datetime\n",
        "\n",
        "import httpx\n",
        "from bs4 import BeautifulSoup\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1. CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "@dataclass\n",
        "class AppConfig:\n",
        "    \"\"\"Centralized configuration. In production, load from env vars / config file.\"\"\"\n",
        "    # Embedding\n",
        "    embedding_model_name: str = \"all-MiniLM-L6-v2\"\n",
        "    chunk_size: int = 500\n",
        "    chunk_overlap: int = 100\n",
        "\n",
        "    # Vector DB\n",
        "    collection_name: str = \"scientific_papers\"\n",
        "    chroma_persist_dir: str = \"./chroma_db\"\n",
        "    similarity_metric: str = \"cosine\"\n",
        "    default_n_results: int = 3\n",
        "\n",
        "    # Scraping (httpx)\n",
        "    request_timeout: float = 15.0\n",
        "    max_concurrent_requests: int = 3\n",
        "    max_retries: int = 3\n",
        "    max_connections: int = 10\n",
        "    max_keepalive_connections: int = 5\n",
        "\n",
        "    # LLM\n",
        "    llm_model: str = \"gpt-3.5-turbo\"\n",
        "    llm_temperature_decompose: float = 0.0\n",
        "    llm_temperature_synthesize: float = 0.2\n",
        "    llm_max_tokens: int = 1000\n",
        "    max_context_chunks: int = 15\n",
        "\n",
        "    # Monitoring\n",
        "    log_level: str = \"INFO\"\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. LOGGING SETUP\n",
        "# ============================================================\n",
        "\n",
        "def setup_logging(config: AppConfig) -> logging.Logger:\n",
        "    \"\"\"Configure structured logging.\"\"\"\n",
        "    logger = logging.getLogger(\"rag_pipeline\")\n",
        "    logger.setLevel(getattr(logging, config.log_level))\n",
        "    if not logger.handlers:\n",
        "        handler = logging.StreamHandler()\n",
        "        formatter = logging.Formatter(\n",
        "            \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
        "            datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        "        )\n",
        "        handler.setFormatter(formatter)\n",
        "        logger.addHandler(handler)\n",
        "    # In production: add RotatingFileHandler, JSON formatter for ELK/Datadog, Sentry\n",
        "    return logger\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. METRICS COLLECTOR\n",
        "# ============================================================\n",
        "\n",
        "@dataclass\n",
        "class PipelineMetrics:\n",
        "    \"\"\"Collects metrics for monitoring/alerting. In production, push to Prometheus/Datadog.\"\"\"\n",
        "    scrape_success: int = 0\n",
        "    scrape_failures: int = 0\n",
        "    chunks_created: int = 0\n",
        "    queries_processed: int = 0\n",
        "    avg_retrieval_time_ms: float = 0.0\n",
        "    avg_llm_latency_ms: float = 0.0\n",
        "    total_tokens_used: int = 0\n",
        "    errors: List[Dict] = field(default_factory=list)\n",
        "\n",
        "    def record_error(self, component: str, error: str):\n",
        "        self.errors.append({\n",
        "            \"timestamp\": datetime.utcnow().isoformat(),\n",
        "            \"component\": component,\n",
        "            \"error\": error,\n",
        "        })\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return {\n",
        "            \"scrape_success\": self.scrape_success,\n",
        "            \"scrape_failures\": self.scrape_failures,\n",
        "            \"chunks_created\": self.chunks_created,\n",
        "            \"queries_processed\": self.queries_processed,\n",
        "            \"total_tokens_used\": self.total_tokens_used,\n",
        "            \"recent_errors\": self.errors[-10:],\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4. INPUT VALIDATION\n",
        "# ============================================================\n",
        "\n",
        "class InputValidator:\n",
        "    \"\"\"Validates and sanitizes user inputs.\"\"\"\n",
        "    MAX_QUERY_LENGTH = 2000\n",
        "    ALLOWED_URL_PATTERNS = [\n",
        "        r\"^https?://arxiv\\.org/\",\n",
        "        r\"^https?://pubmed\\.ncbi\\.nlm\\.nih\\.gov/\",\n",
        "    ]\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_query(query: str) -> str:\n",
        "        if not query or not query.strip():\n",
        "            raise ValueError(\"Query cannot be empty\")\n",
        "        query = query.strip()\n",
        "        if len(query) > InputValidator.MAX_QUERY_LENGTH:\n",
        "            raise ValueError(f\"Query exceeds max length of {InputValidator.MAX_QUERY_LENGTH}\")\n",
        "        query = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]', '', query)\n",
        "        return query\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_url(url: str) -> bool:\n",
        "        return any(re.match(p, url) for p in InputValidator.ALLOWED_URL_PATTERNS)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. PRODUCTION RAG PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "class ProductionRAGPipeline:\n",
        "    \"\"\"\n",
        "    Production-grade RAG pipeline for scientific literature search.\n",
        "\n",
        "    Features:\n",
        "    - Async HTTP via httpx with connection pooling & transport-level retries\n",
        "    - Structured logging at every stage\n",
        "    - Input validation and sanitization\n",
        "    - Metrics collection for monitoring\n",
        "    - Graceful error handling and degradation\n",
        "    - Idempotent upserts (safe to re-run ingestion)\n",
        "    - Configurable via AppConfig\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Optional[AppConfig] = None):\n",
        "        self.config = config or AppConfig()\n",
        "        self.logger = setup_logging(self.config)\n",
        "        self.metrics = PipelineMetrics()\n",
        "        self.validator = InputValidator()\n",
        "\n",
        "        self.logger.info(\"Initializing RAG Pipeline...\")\n",
        "\n",
        "        try:\n",
        "            self.embedding_model = SentenceTransformer(self.config.embedding_model_name)\n",
        "            self.logger.info(f\"Loaded embedding model: {self.config.embedding_model_name}\")\n",
        "        except Exception as e:\n",
        "            self.logger.critical(f\"Failed to load embedding model: {e}\")\n",
        "            raise\n",
        "\n",
        "        try:\n",
        "            self.chroma_client = chromadb.PersistentClient(path=self.config.chroma_persist_dir)\n",
        "            self.collection = self.chroma_client.get_or_create_collection(\n",
        "                name=self.config.collection_name,\n",
        "                metadata={\"hnsw:space\": self.config.similarity_metric}\n",
        "            )\n",
        "            self.logger.info(f\"ChromaDB ready. {self.collection.count()} documents\")\n",
        "        except Exception as e:\n",
        "            self.logger.critical(f\"Failed to initialize ChromaDB: {e}\")\n",
        "            raise\n",
        "\n",
        "        self.llm_client = OpenAI()\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.config.chunk_size,\n",
        "            chunk_overlap=self.config.chunk_overlap,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "        )\n",
        "\n",
        "        # httpx transport with built-in retries\n",
        "        self._http_transport = httpx.AsyncHTTPTransport(retries=self.config.max_retries)\n",
        "        self._http_limits = httpx.Limits(\n",
        "            max_connections=self.config.max_connections,\n",
        "            max_keepalive_connections=self.config.max_keepalive_connections,\n",
        "        )\n",
        "\n",
        "        self.logger.info(\"RAG Pipeline initialized successfully\")\n",
        "\n",
        "    def _get_http_client(self) -> httpx.AsyncClient:\n",
        "        \"\"\"Create a configured httpx async client. Use as async context manager.\"\"\"\n",
        "        return httpx.AsyncClient(\n",
        "            transport=self._http_transport,\n",
        "            limits=self._http_limits,\n",
        "            timeout=httpx.Timeout(self.config.request_timeout),\n",
        "            follow_redirects=True,\n",
        "            headers={\"User-Agent\": \"Mozilla/5.0 (Scientific RAG Bot)\"},\n",
        "        )\n",
        "\n",
        "    # ---- INGESTION ----\n",
        "\n",
        "    async def ingest_urls(self, urls: List[str]) -> Dict[str, int]:\n",
        "        \"\"\"Ingest multiple URLs concurrently with rate limiting. Returns {url: chunk_count}.\"\"\"\n",
        "        results = {}\n",
        "        semaphore = asyncio.Semaphore(self.config.max_concurrent_requests)\n",
        "\n",
        "        async with self._get_http_client() as client:\n",
        "            async def _ingest_one(url: str):\n",
        "                async with semaphore:\n",
        "                    try:\n",
        "                        n = await self._ingest_single_url(client, url)\n",
        "                        results[url] = n\n",
        "                    except Exception as e:\n",
        "                        self.logger.error(f\"Failed to ingest {url}: {e}\")\n",
        "                        results[url] = 0\n",
        "\n",
        "            await asyncio.gather(*[_ingest_one(url) for url in urls])\n",
        "        return results\n",
        "\n",
        "    async def _ingest_single_url(self, client: httpx.AsyncClient, url: str) -> int:\n",
        "        self.logger.info(f\"Ingesting URL: {url}\")\n",
        "        if not self.validator.validate_url(url):\n",
        "            raise ValueError(f\"URL not from an allowed source: {url}\")\n",
        "\n",
        "        try:\n",
        "            response = await client.get(url)\n",
        "            response.raise_for_status()\n",
        "            self.metrics.scrape_success += 1\n",
        "        except httpx.HTTPStatusError as e:\n",
        "            self.metrics.scrape_failures += 1\n",
        "            self.metrics.record_error(\"scraper\", f\"HTTP {e.response.status_code}\")\n",
        "            raise\n",
        "        except httpx.RequestError as e:\n",
        "            self.metrics.scrape_failures += 1\n",
        "            self.metrics.record_error(\"scraper\", str(e))\n",
        "            raise\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        if \"arxiv.org\" in url:\n",
        "            doc = self._parse_arxiv(soup, url)\n",
        "        elif \"pubmed\" in url:\n",
        "            doc = self._parse_pubmed(soup, url)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported source: {url}\")\n",
        "\n",
        "        if not doc[\"content\"]:\n",
        "            self.logger.warning(f\"No content extracted from {url}\")\n",
        "            return 0\n",
        "\n",
        "        chunks = self._create_chunks(doc)\n",
        "        self._store_chunks(chunks)\n",
        "        self.logger.info(f\"Ingested {len(chunks)} chunks from {doc['title'][:50]}\")\n",
        "        return len(chunks)\n",
        "\n",
        "    def _parse_arxiv(self, soup, url):\n",
        "        title_tag = soup.find(\"h1\", class_=\"title\")\n",
        "        title = title_tag.get_text(strip=True).replace(\"Title:\", \"\").strip() if title_tag else \"Unknown\"\n",
        "        abstract_tag = soup.find(\"blockquote\", class_=\"abstract\")\n",
        "        content = abstract_tag.get_text(strip=True).replace(\"Abstract:\", \"\").strip() if abstract_tag else \"\"\n",
        "        return {\"title\": title, \"url\": url, \"content\": content, \"source\": \"arxiv\"}\n",
        "\n",
        "    def _parse_pubmed(self, soup, url):\n",
        "        title_tag = soup.find(\"h1\", class_=\"heading-title\")\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"Unknown\"\n",
        "        abstract_tag = soup.find(\"div\", class_=\"abstract-content\")\n",
        "        content = abstract_tag.get_text(strip=True) if abstract_tag else \"\"\n",
        "        return {\"title\": title, \"url\": url, \"content\": content, \"source\": \"pubmed\"}\n",
        "\n",
        "    def _create_chunks(self, doc):\n",
        "        text = re.sub(r'\\s+', ' ', doc[\"content\"]).strip()\n",
        "        raw_chunks = self.text_splitter.split_text(text)\n",
        "        chunks = []\n",
        "        for i, chunk_text in enumerate(raw_chunks):\n",
        "            chunk_id = hashlib.md5(f\"{doc['url']}_{i}\".encode()).hexdigest()\n",
        "            chunks.append({\n",
        "                \"id\": chunk_id,\n",
        "                \"text\": chunk_text,\n",
        "                \"metadata\": {\n",
        "                    \"source_url\": doc[\"url\"],\n",
        "                    \"title\": doc[\"title\"],\n",
        "                    \"source\": doc[\"source\"],\n",
        "                    \"chunk_index\": i,\n",
        "                    \"total_chunks\": len(raw_chunks),\n",
        "                    \"ingested_at\": datetime.utcnow().isoformat(),\n",
        "                }\n",
        "            })\n",
        "        self.metrics.chunks_created += len(chunks)\n",
        "        return chunks\n",
        "\n",
        "    def _store_chunks(self, chunks):\n",
        "        \"\"\"Embed and store. Uses upsert for idempotency.\"\"\"\n",
        "        texts = [c[\"text\"] for c in chunks]\n",
        "        ids = [c[\"id\"] for c in chunks]\n",
        "        metadatas = [c[\"metadata\"] for c in chunks]\n",
        "        embeddings = self.embedding_model.encode(texts).tolist()\n",
        "        self.collection.upsert(documents=texts, embeddings=embeddings, metadatas=metadatas, ids=ids)\n",
        "\n",
        "    # ---- QUERY ----\n",
        "\n",
        "    def query(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"Full RAG pipeline: validate â†’ decompose â†’ retrieve â†’ synthesize.\"\"\"\n",
        "        start_time = time.time()\n",
        "        self.logger.info(f\"Processing query: {question[:100]}\")\n",
        "        question = self.validator.validate_query(question)\n",
        "\n",
        "        # Step 1: Decompose (graceful degradation on failure)\n",
        "        try:\n",
        "            sub_questions = self._decompose_question(question)\n",
        "            self.logger.info(f\"Decomposed into {len(sub_questions)} sub-questions\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Decomposition failed, using original: {e}\")\n",
        "            self.metrics.record_error(\"decomposer\", str(e))\n",
        "            sub_questions = [question]\n",
        "\n",
        "        # Step 2: Retrieve\n",
        "        try:\n",
        "            context_chunks = self._retrieve(sub_questions)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Retrieval failed: {e}\")\n",
        "            return {\"answer\": \"Error retrieving information.\", \"error\": str(e)}\n",
        "\n",
        "        if not context_chunks:\n",
        "            return {\"answer\": \"No relevant information found.\", \"context_count\": 0}\n",
        "\n",
        "        # Step 3: Synthesize (returns raw context on failure)\n",
        "        try:\n",
        "            answer = self._synthesize(question, sub_questions, context_chunks)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Synthesis failed: {e}\")\n",
        "            return {\n",
        "                \"answer\": \"Found relevant data but failed to generate answer.\",\n",
        "                \"context_chunks\": [c[\"text\"][:200] for c in context_chunks],\n",
        "            }\n",
        "\n",
        "        total_ms = (time.time() - start_time) * 1000\n",
        "        self.metrics.queries_processed += 1\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sub_questions\": sub_questions,\n",
        "            \"sources\": list(set(c[\"metadata\"][\"source_url\"] for c in context_chunks)),\n",
        "            \"context_count\": len(context_chunks),\n",
        "            \"latency_ms\": total_ms,\n",
        "        }\n",
        "\n",
        "    def _decompose_question(self, question):\n",
        "        prompt = f\"\"\"Break this question into 2-5 simpler, searchable sub-questions.\n",
        "Return ONLY a JSON list of strings.\n",
        "\n",
        "Question: \\\"{question}\\\"\n",
        "\n",
        "Sub-questions:\"\"\"\n",
        "        response = self.llm_client.chat.completions.create(\n",
        "            model=self.config.llm_model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=self.config.llm_temperature_decompose, max_tokens=500,\n",
        "        )\n",
        "        raw = response.choices[0].message.content.strip()\n",
        "        try:\n",
        "            return json.loads(raw)\n",
        "        except json.JSONDecodeError:\n",
        "            match = re.search(r'\\[.*\\]', raw, re.DOTALL)\n",
        "            return json.loads(match.group()) if match else [question]\n",
        "\n",
        "    def _retrieve(self, sub_questions, n_per_query=3):\n",
        "        all_results, seen = [], set()\n",
        "        for sq in sub_questions:\n",
        "            emb = self.embedding_model.encode([sq]).tolist()\n",
        "            results = self.collection.query(query_embeddings=emb, n_results=n_per_query,\n",
        "                                            include=[\"documents\", \"metadatas\", \"distances\"])\n",
        "            for i in range(len(results[\"ids\"][0])):\n",
        "                doc_id = results[\"ids\"][0][i]\n",
        "                if doc_id not in seen:\n",
        "                    seen.add(doc_id)\n",
        "                    all_results.append({\n",
        "                        \"text\": results[\"documents\"][0][i],\n",
        "                        \"metadata\": results[\"metadatas\"][0][i],\n",
        "                        \"distance\": results[\"distances\"][0][i],\n",
        "                    })\n",
        "        return all_results[:self.config.max_context_chunks]\n",
        "\n",
        "    def _synthesize(self, question, sub_questions, chunks):\n",
        "        ctx = \"\\n\\n\".join(f\"[Source: {c['metadata']['title']}]\\n{c['text']}\" for c in chunks)\n",
        "        prompt = f\"\"\"Answer based ONLY on the provided context. If insufficient, say so.\n",
        "\n",
        "Sub-questions: {json.dumps(sub_questions)}\n",
        "\n",
        "Context:\\n{ctx}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer with citations:\"\"\"\n",
        "        response = self.llm_client.chat.completions.create(\n",
        "            model=self.config.llm_model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=self.config.llm_temperature_synthesize,\n",
        "            max_tokens=self.config.llm_max_tokens,\n",
        "        )\n",
        "        if response.usage:\n",
        "            self.metrics.total_tokens_used += response.usage.total_tokens\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    # ---- HEALTH CHECK ----\n",
        "\n",
        "    def health_check(self) -> Dict[str, Any]:\n",
        "        \"\"\"Health check for k8s / load balancer probes.\"\"\"\n",
        "        checks = {}\n",
        "        try:\n",
        "            checks[\"vector_db\"] = {\"status\": \"healthy\", \"count\": self.collection.count()}\n",
        "        except Exception as e:\n",
        "            checks[\"vector_db\"] = {\"status\": \"unhealthy\", \"error\": str(e)}\n",
        "        try:\n",
        "            self.embedding_model.encode([\"test\"])\n",
        "            checks[\"embedding_model\"] = {\"status\": \"healthy\"}\n",
        "        except Exception as e:\n",
        "            checks[\"embedding_model\"] = {\"status\": \"unhealthy\", \"error\": str(e)}\n",
        "        checks[\"metrics\"] = self.metrics.to_dict()\n",
        "        checks[\"overall\"] = \"healthy\" if all(\n",
        "            v.get(\"status\") == \"healthy\" for v in checks.values() if isinstance(v, dict) and \"status\" in v\n",
        "        ) else \"degraded\"\n",
        "        return checks\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# USAGE\n",
        "# ============================================================\n",
        "config = AppConfig(log_level=\"INFO\")\n",
        "pipeline = ProductionRAGPipeline(config)\n",
        "\n",
        "ingestion_results = await pipeline.ingest_urls(ARXIV_URLS + PUBMED_URLS)\n",
        "for url, count in ingestion_results.items():\n",
        "    print(f\"  {url}: {count} chunks\")\n",
        "\n",
        "result = pipeline.query(\"What are the dangerous medical conditions AI might misdiagnose?\")\n",
        "print(f\"\\nAnswer: {result['answer']}\")\n",
        "print(f\"Sources: {result.get('sources', [])}\")\n",
        "print(f\"Latency: {result.get('latency_ms', 0):.0f}ms\")\n",
        "print(f\"\\nHealth: {json.dumps(pipeline.health_check(), indent=2)}\")"
      ],
      "metadata": {
        "id": "qcm6HPB2HRBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of Production Hardening Applied:\n",
        "\n",
        "| Category | Implementation |\n",
        "|----------|---------------|\n",
        "| **Logging** | Structured logging with levels, timestamps, component names |\n",
        "| **Error Handling** | Try/catch at every stage with graceful degradation |\n",
        "| **Retries** | httpx `AsyncHTTPTransport(retries=3)` â€” transport-level retries |\n",
        "| **Connection Pooling** | httpx `Limits` for max connections and keepalive management |\n",
        "| **Async I/O** | httpx `AsyncClient` with semaphore-based concurrency control |\n",
        "| **Configuration** | Centralized `AppConfig` dataclass, no hardcoded values |\n",
        "| **Input Validation** | Query length limits, URL allowlisting, control char sanitization |\n",
        "| **Idempotency** | `upsert` instead of `add` (safe to re-run ingestion) |\n",
        "| **Metrics** | `PipelineMetrics` tracks success/failures, latency, tokens, errors |\n",
        "| **Health Checks** | `health_check()` for k8s/load balancer probes |\n",
        "| **Observability** | Full query tracing (sub-questions, sources, latency in response) |\n",
        "| **Persistent Storage** | ChromaDB with persistent directory (survives restarts) |\n",
        "\n",
        "**Additional items for real production (not implemented but important):**\n",
        "- API rate limiting (slowapi / API gateway)\n",
        "- Auth (API keys, JWT)\n",
        "- Caching (Redis) for repeated queries\n",
        "- Async task queue (Celery) for ingestion\n",
        "- Docker + k8s orchestration\n",
        "- CI/CD with automated tests\n",
        "- Secrets management (Vault, AWS Secrets Manager)\n",
        "- Backups and disaster recovery\n",
        "- Load testing"
      ],
      "metadata": {
        "id": "OS-msv3RHUs5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0gz3ekLuHREg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Task 4: Evaluation â€” Measuring RAG Quality\n",
        "\n",
        "### Evaluation Strategy\n",
        "\n",
        "RAG systems have multiple components that can fail independently, so we need **multi-dimensional evaluation**:\n",
        "\n",
        "#### 1. Retrieval Quality (Does the vector DB return relevant chunks?)\n",
        "- **Metric**: Retrieval Precision@K\n",
        "- **How**: For each query, ask an LLM judge if each retrieved chunk is relevant\n",
        "\n",
        "#### 2. Answer Faithfulness (Is the answer grounded in the retrieved context?)\n",
        "- **Metric**: Faithfulness Score (LLM-as-Judge)\n",
        "- **How**: Verify every claim in the answer is supported by the context. Catches hallucinations.\n",
        "\n",
        "#### 3. Answer Relevance (Does the answer address the question?)\n",
        "- **Metric**: Relevance Score (LLM-as-Judge)\n",
        "- **How**: Rate how well the answer addresses the original question (0-1)\n",
        "\n",
        "#### 4. Answer Correctness (Is the answer factually correct?)\n",
        "- **Metric**: Correctness Score against gold-standard answers\n",
        "- **How**: Compare generated answer to a reference answer\n",
        "\n",
        "#### Why this approach?\n",
        "- **Retrieval metrics** isolate search quality from generation quality\n",
        "- **Faithfulness** catches the #1 RAG failure mode: hallucination\n",
        "- **Relevance** ensures the system answers what was asked\n",
        "- **Correctness** is ground truth validation (expensive but essential)\n",
        "\n",
        "This follows the **RAGAS framework** (Retrieval Augmented Generation Assessment)."
      ],
      "metadata": {
        "id": "TiqHXm4WHmm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGEvaluator:\n",
        "    \"\"\"\n",
        "    Evaluates RAG pipeline quality across multiple dimensions.\n",
        "\n",
        "    Metrics:\n",
        "    1. Retrieval Precision@K - Are retrieved docs relevant?\n",
        "    2. Faithfulness - Is the answer grounded in context?\n",
        "    3. Answer Relevance - Does the answer address the question?\n",
        "    4. Answer Correctness - Is the answer factually right?\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm_client: OpenAI, model: str = \"gpt-3.5-turbo\"):\n",
        "        self.llm_client = llm_client\n",
        "        self.model = model\n",
        "\n",
        "    def _llm_judge(self, prompt: str) -> str:\n",
        "        response = self.llm_client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.0,\n",
        "            max_tokens=500,\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    def evaluate_retrieval_precision(self, query: str, retrieved_chunks: List[Dict]) -> float:\n",
        "        \"\"\"Use LLM-as-judge to check relevance of each retrieved chunk.\"\"\"\n",
        "        if not retrieved_chunks:\n",
        "            return 0.0\n",
        "        relevant = 0\n",
        "        for chunk in retrieved_chunks:\n",
        "            prompt = f\"\"\"Is this passage relevant to the query?\n",
        "Query: \"{query}\"\n",
        "Passage: \"{chunk['text'][:500]}\"\n",
        "Answer ONLY 'yes' or 'no'.\"\"\"\n",
        "            if \"yes\" in self._llm_judge(prompt).lower():\n",
        "                relevant += 1\n",
        "        return relevant / len(retrieved_chunks)\n",
        "\n",
        "    def evaluate_faithfulness(self, answer: str, context_chunks: List[Dict]) -> Tuple[float, str]:\n",
        "        \"\"\"Check if the answer is grounded in the provided context. Catches hallucinations.\"\"\"\n",
        "        context_text = \"\\n\\n\".join([c[\"text\"] for c in context_chunks])\n",
        "        prompt = f\"\"\"Determine if the answer is fully supported by the context.\n",
        "\n",
        "Context:\n",
        "{context_text[:3000]}\n",
        "\n",
        "Answer:\n",
        "{answer}\n",
        "\n",
        "Evaluate: list claims, check support, give score 0.0-1.0.\n",
        "Respond in JSON: {{\"claims\": [{{\"claim\": \"...\", \"supported\": true/false}}], \"score\": X.X, \"explanation\": \"...\"}}\"\"\"\n",
        "\n",
        "        result = self._llm_judge(prompt)\n",
        "        try:\n",
        "            parsed = json.loads(result)\n",
        "            return parsed.get(\"score\", 0.0), parsed.get(\"explanation\", \"\")\n",
        "        except json.JSONDecodeError:\n",
        "            match = re.search(r'(\\d\\.\\d+)', result)\n",
        "            return (float(match.group(1)) if match else 0.5), result[:200]\n",
        "\n",
        "    def evaluate_relevance(self, question: str, answer: str) -> Tuple[float, str]:\n",
        "        \"\"\"Rate how well the answer addresses the question.\"\"\"\n",
        "        prompt = f\"\"\"Rate how well this answer addresses the question.\n",
        "Question: \"{question}\"\n",
        "Answer: \"{answer}\"\n",
        "Score 0.0-1.0. Respond in JSON: {{\"score\": X.X, \"explanation\": \"...\"}}\"\"\"\n",
        "\n",
        "        result = self._llm_judge(prompt)\n",
        "        try:\n",
        "            parsed = json.loads(result)\n",
        "            return parsed.get(\"score\", 0.0), parsed.get(\"explanation\", \"\")\n",
        "        except json.JSONDecodeError:\n",
        "            match = re.search(r'(\\d\\.\\d+)', result)\n",
        "            return (float(match.group(1)) if match else 0.5), result[:200]\n",
        "\n",
        "    def evaluate_correctness(self, question: str, answer: str, reference: str) -> Tuple[float, str]:\n",
        "        \"\"\"Compare generated answer against a gold-standard reference.\"\"\"\n",
        "        prompt = f\"\"\"Compare the generated answer to the reference for correctness.\n",
        "Question: \"{question}\"\n",
        "Reference: \"{reference}\"\n",
        "Generated: \"{answer}\"\n",
        "Score 0.0-1.0. Consider: accuracy, completeness, absence of false claims.\n",
        "Respond in JSON: {{\"score\": X.X, \"explanation\": \"...\"}}\"\"\"\n",
        "\n",
        "        result = self._llm_judge(prompt)\n",
        "        try:\n",
        "            parsed = json.loads(result)\n",
        "            return parsed.get(\"score\", 0.0), parsed.get(\"explanation\", \"\")\n",
        "        except json.JSONDecodeError:\n",
        "            match = re.search(r'(\\d\\.\\d+)', result)\n",
        "            return (float(match.group(1)) if match else 0.5), result[:200]\n",
        "\n",
        "    def evaluate_full(self, question, answer, context_chunks, reference_answer=None):\n",
        "        \"\"\"Run all evaluation metrics on a single query-answer pair.\"\"\"\n",
        "        results = {}\n",
        "        results[\"retrieval_precision\"] = self.evaluate_retrieval_precision(question, context_chunks)\n",
        "\n",
        "        faith_score, faith_exp = self.evaluate_faithfulness(answer, context_chunks)\n",
        "        results[\"faithfulness\"] = {\"score\": faith_score, \"explanation\": faith_exp}\n",
        "\n",
        "        rel_score, rel_exp = self.evaluate_relevance(question, answer)\n",
        "        results[\"relevance\"] = {\"score\": rel_score, \"explanation\": rel_exp}\n",
        "\n",
        "        scores = [results[\"retrieval_precision\"], faith_score, rel_score]\n",
        "\n",
        "        if reference_answer:\n",
        "            corr_score, corr_exp = self.evaluate_correctness(question, answer, reference_answer)\n",
        "            results[\"correctness\"] = {\"score\": corr_score, \"explanation\": corr_exp}\n",
        "            scores.append(corr_score)\n",
        "\n",
        "        results[\"composite_score\"] = sum(scores) / len(scores)\n",
        "        return results\n",
        "\n",
        "\n",
        "print(\"RAGEvaluator class defined.\")"
      ],
      "metadata": {
        "id": "CwOrEYN1RPGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Run evaluation on test cases\n",
        "# ============================================================\n",
        "\n",
        "test_cases = [\n",
        "    {\n",
        "        \"question\": \"What are the risks of using AI for medical diagnosis?\",\n",
        "        \"reference_answer\": (\n",
        "            \"Key risks include: hallucination of medical facts, bias in training data \"\n",
        "            \"leading to disparate outcomes, lack of explainability, automation bias, \"\n",
        "            \"and misdiagnosis of rare conditions. Safety measures include human oversight, \"\n",
        "            \"clinical validation, and demographic performance monitoring.\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How do LLMs perform on medical licensing exams?\",\n",
        "        \"reference_answer\": (\n",
        "            \"LLMs like GPT-4 and Med-PaLM have achieved passing scores on USMLE, \"\n",
        "            \"with some scoring above average. However, exam performance does not \"\n",
        "            \"directly translate to clinical competence.\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What NLP techniques are used in biomedical text mining?\",\n",
        "        \"reference_answer\": None  # No reference â€” evaluates retrieval, faithfulness, relevance only\n",
        "    },\n",
        "]\n",
        "\n",
        "evaluator = RAGEvaluator(llm_client=OpenAI())\n",
        "all_eval_results = []\n",
        "\n",
        "for tc in test_cases:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluating: {tc['question'][:60]}...\")\n",
        "\n",
        "    sub_questions = decompose_question(tc[\"question\"])\n",
        "    context_chunks = retrieve_for_subquestions(sub_questions)\n",
        "    answer = synthesize_answer(tc[\"question\"], sub_questions, context_chunks)\n",
        "\n",
        "    eval_result = evaluator.evaluate_full(\n",
        "        question=tc[\"question\"],\n",
        "        answer=answer,\n",
        "        context_chunks=context_chunks,\n",
        "        reference_answer=tc.get(\"reference_answer\"),\n",
        "    )\n",
        "    eval_result[\"question\"] = tc[\"question\"]\n",
        "    all_eval_results.append(eval_result)\n",
        "\n",
        "    print(f\"  Retrieval Precision: {eval_result['retrieval_precision']:.2f}\")\n",
        "    print(f\"  Faithfulness:        {eval_result['faithfulness']['score']:.2f}\")\n",
        "    print(f\"  Relevance:           {eval_result['relevance']['score']:.2f}\")\n",
        "    if \"correctness\" in eval_result:\n",
        "        print(f\"  Correctness:         {eval_result['correctness']['score']:.2f}\")\n",
        "    print(f\"  Composite Score:     {eval_result['composite_score']:.2f}\")"
      ],
      "metadata": {
        "id": "wn5tk4aoHy3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary report\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATION SUMMARY REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "n = len(all_eval_results)\n",
        "avg_prec = sum(r[\"retrieval_precision\"] for r in all_eval_results) / n\n",
        "avg_faith = sum(r[\"faithfulness\"][\"score\"] for r in all_eval_results) / n\n",
        "avg_rel = sum(r[\"relevance\"][\"score\"] for r in all_eval_results) / n\n",
        "avg_comp = sum(r[\"composite_score\"] for r in all_eval_results) / n\n",
        "\n",
        "corr_results = [r for r in all_eval_results if \"correctness\" in r]\n",
        "avg_corr = sum(r[\"correctness\"][\"score\"] for r in corr_results) / len(corr_results) if corr_results else None\n",
        "\n",
        "print(f\"\\nTest cases: {n}\")\n",
        "print(f\"  Retrieval Precision@K : {avg_prec:.2f}\")\n",
        "print(f\"  Faithfulness          : {avg_faith:.2f}\")\n",
        "print(f\"  Answer Relevance      : {avg_rel:.2f}\")\n",
        "if avg_corr is not None:\n",
        "    print(f\"  Answer Correctness    : {avg_corr:.2f} ({len(corr_results)} with references)\")\n",
        "print(f\"  Composite Score       : {avg_comp:.2f}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"INTERPRETATION:\")\n",
        "print(\"  0.8-1.0 : Excellent â€” production ready\")\n",
        "print(\"  0.6-0.8 : Good â€” minor improvements needed\")\n",
        "print(\"  0.4-0.6 : Fair â€” significant improvements needed\")\n",
        "print(\"  0.0-0.4 : Poor â€” fundamental issues\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "id": "MiSs5ZnyH1Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Design Rationale\n",
        "\n",
        "**Why LLM-as-Judge?**\n",
        "- BLEU/ROUGE measure surface text overlap â€” poor proxy for RAG answer quality\n",
        "- LLM-as-Judge correlates much better with human judgment\n",
        "- Cost-effective compared to human evaluation at scale\n",
        "\n",
        "**Why these 4 metrics?**\n",
        "1. **Retrieval Precision** â†’ Isolates vector DB quality from LLM quality\n",
        "2. **Faithfulness** â†’ The #1 RAG failure mode is hallucination. This directly measures it.\n",
        "3. **Relevance** â†’ Ensures the system answers what was asked\n",
        "4. **Correctness** â†’ Ground truth validation where available\n",
        "\n",
        "**Tradeoffs acknowledged:**\n",
        "- LLM-as-Judge has its own biases (prefers longer answers, own style)\n",
        "- Gold-standard test sets are expensive but essential\n",
        "- In production: add human eval on sample + automated regression tests\n",
        "- Also track: latency, cost/query, user satisfaction (thumbs up/down)"
      ],
      "metadata": {
        "id": "LET2FDECH4wf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JiXrjlx5H5JA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}